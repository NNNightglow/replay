#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¸‚åœºå…ƒæ•°æ®ç®¡ç†å™¨

è´Ÿè´£ç®¡ç†å¸‚åœºæƒ…ç»ªæŒ‡æ ‡ï¼Œå¦‚çº¢ç›˜ç‡ã€æ¶¨åœæ•°ã€è·Œåœæ•°ã€åœ°å¤©æ¿ä¸ªæ•°ç­‰
"""

import polars as pl
from datetime import datetime, timedelta, date
from pathlib import Path
import os
import time
from typing import Optional, Dict, List, Tuple, Union
import akshare as ak
import pandas as pd
import numpy as np
import threading
import tempfile
import shutil

try:
    import fcntl
except ImportError:
    fcntl = None  # Windows doesn't have fcntl

# å…¨å±€é”ï¼Œé˜²æ­¢å¹¶å‘å†™å…¥
_file_locks = {}
_lock_mutex = threading.Lock()

def _parse_to_date(value):
    """å°†å¯èƒ½ä¸º str/datetime/date çš„å€¼å®‰å…¨è½¬æ¢ä¸º datetime.dateã€‚

    è¿”å›å€¼ï¼šdatetime.date æˆ– None
    """
    if value is None:
        return None
    if isinstance(value, date):
        return value
    if isinstance(value, datetime):
        return value.date()
    if isinstance(value, str):
        for fmt in ('%Y-%m-%d', '%Y/%m/%d', '%Y%m%d'):
            try:
                return datetime.strptime(value, fmt).date()
            except ValueError:
                continue
        return None
    return None

def _ensure_date_column(df: pl.DataFrame, column_name: str = 'æ—¥æœŸ') -> pl.DataFrame:
    """ç¡®ä¿ DataFrame æŒ‡å®šåˆ—ä¸º pl.Date ç±»å‹ã€‚

    è‹¥ä¸º Utf8ï¼Œå°è¯•æŒ‰å¸¸è§ä¸‰ç§æ ¼å¼è§£æï¼›è‹¥ä¸º Datetime åˆ™è½¬ä¸º Dateï¼›è‹¥å·²ä¸º Date ç›´æ¥è¿”å›ã€‚
    """
    if df is None or df.is_empty() or column_name not in df.columns:
        return df
    dtype = df.schema.get(column_name)
    try:
        if dtype == pl.Date:
            return df
        if dtype == pl.Datetime:
            return df.with_columns([
                pl.col(column_name).cast(pl.Date).alias(column_name)
            ])
        if dtype == pl.Utf8:
            # ä¾æ¬¡å°è¯•å¤šç§æ—¥æœŸæ ¼å¼å¹¶åˆå¹¶
            parsed1 = pl.col(column_name).str.strptime(pl.Date, fmt='%Y-%m-%d', strict=False)
            parsed2 = pl.col(column_name).str.strptime(pl.Date, fmt='%Y/%m/%d', strict=False)
            parsed3 = pl.col(column_name).str.strptime(pl.Date, fmt='%Y%m%d', strict=False)
            return df.with_columns([
                pl.coalesce([parsed1, parsed2, parsed3]).alias(column_name)
            ])
        # å…¶ä»–ç±»å‹å°½é‡ç›´æ¥ cast
        return df.with_columns([
            pl.col(column_name).cast(pl.Date).alias(column_name)
        ])
    except Exception:
        # å…œåº•ï¼šç”¨ Python è§£æ
        values = [
            _parse_to_date(v) for v in df[column_name].to_list()
        ]
        return df.with_columns([
            pl.Series(name=column_name, values=values).cast(pl.Date)
        ])

def calculate_stock_indicators(df: pl.DataFrame) -> pl.DataFrame:
    """
    è®¡ç®—è‚¡ç¥¨æŠ€æœ¯æŒ‡æ ‡ï¼šæ¶¨è·Œå¹…å’Œç§»åŠ¨å‡çº¿
    
    å‚æ•°:
        df: åŒ…å«è‚¡ç¥¨æ•°æ®çš„DataFrameï¼Œå¿…é¡»åŒ…å«ä»¥ä¸‹åˆ—ï¼š
            - date: æ—¥æœŸ
            - æ”¶ç›˜: æ”¶ç›˜ä»· (f64)
            - åç§°: è‚¡ç¥¨åç§° (str)
    
    è¿”å›:
        æ·»åŠ äº†æŠ€æœ¯æŒ‡æ ‡çš„DataFrame
    """
    
    # æ•°æ®éªŒè¯
    required_cols = ['æ—¥æœŸ', 'æ”¶ç›˜', 'åç§°']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_cols}")
    
    # ç¡®ä¿æ•°æ®æŒ‰è‚¡ç¥¨å’Œæ—¥æœŸæ’åº
    df_sorted = df.sort(['åç§°', 'æ—¥æœŸ'])
    
    # è®¡ç®—æ¶¨è·Œå¹…ï¼ˆåŸºäºæ”¶ç›˜ä»·ï¼‰
    df_with_changes = df_sorted.with_columns([
        # 5æ—¥æ¶¨è·Œå¹…
        ((pl.col('æ”¶ç›˜') / pl.col('æ”¶ç›˜').shift(5).over('åç§°') - 1) * 100)
        .round(2)
        .alias('5æ—¥æ¶¨è·Œå¹…'),
        
        # 10æ—¥æ¶¨è·Œå¹…
        ((pl.col('æ”¶ç›˜') / pl.col('æ”¶ç›˜').shift(10).over('åç§°') - 1) * 100)
        .round(2)
        .alias('10æ—¥æ¶¨è·Œå¹…'),
        
        # 20æ—¥æ¶¨è·Œå¹…
        ((pl.col('æ”¶ç›˜') / pl.col('æ”¶ç›˜').shift(20).over('åç§°') - 1) * 100)
        .round(2)
        .alias('20æ—¥æ¶¨è·Œå¹…')
    ])
    
    # è®¡ç®—ç§»åŠ¨å‡çº¿
    df_with_ma = df_with_changes.with_columns([
        # 5æ—¥å‡çº¿
        pl.col('æ”¶ç›˜')
        .rolling_mean(window_size=5, min_periods=1)
        .over('åç§°')
        .round(2)
        .alias('MA5'),
        
        # 10æ—¥å‡çº¿
        pl.col('æ”¶ç›˜')
        .rolling_mean(window_size=10, min_periods=1)
        .over('åç§°')
        .round(2)
        .alias('MA10'),
        
        # 20æ—¥å‡çº¿
        pl.col('æ”¶ç›˜')
        .rolling_mean(window_size=20, min_periods=1)
        .over('åç§°')
        .round(2)
        .alias('MA20'),
        
    ])
    
    return df_with_ma

def add_price_relative_indicators(df: pl.DataFrame) -> pl.DataFrame:
    """
    æ·»åŠ ä»·æ ¼ç›¸å¯¹æŒ‡æ ‡ï¼ˆç›¸å¯¹äºå‡çº¿çš„ä½ç½®ï¼‰
    
    å‚æ•°:
        df: å·²è®¡ç®—å‡çº¿çš„DataFrame
    
    è¿”å›:
        æ·»åŠ äº†ç›¸å¯¹æŒ‡æ ‡çš„DataFrame
    """
    return df.with_columns([
        # æ”¶ç›˜ä»·ç›¸å¯¹äºå„å‡çº¿çš„ä½ç½®ï¼ˆç™¾åˆ†æ¯”ï¼‰
        ((pl.col('æ”¶ç›˜') / pl.col('MA5') - 1) * 100).round(2).alias('ç›¸å¯¹MA5'),
        ((pl.col('æ”¶ç›˜') / pl.col('MA10') - 1) * 100).round(2).alias('ç›¸å¯¹MA10'),
        ((pl.col('æ”¶ç›˜') / pl.col('MA20') - 1) * 100).round(2).alias('ç›¸å¯¹MA20'),
        
        
        # å‡çº¿æ’åˆ—çŠ¶æ€
        pl.when((pl.col('MA5') > pl.col('MA10')) & 
                (pl.col('MA10') > pl.col('MA20')))
        .then(pl.lit("å¤šå¤´æ’åˆ—"))
        .when((pl.col('MA5') < pl.col('MA10')) & 
              (pl.col('MA10') < pl.col('MA20')))
        .then(pl.lit("ç©ºå¤´æ’åˆ—"))
        .otherwise(pl.lit("å‡çº¿æ··ä¹±"))
        .alias('å‡çº¿æ’åˆ—')
    ])

def add_candlestick_trend_streaks(df: pl.DataFrame) -> pl.DataFrame:
    """
    è®¡ç®—Kçº¿è¶‹åŠ¿æŒ‡æ ‡ï¼š
    - é˜³çº¿å®šä¹‰ï¼šæ”¶ç›˜ > å¼€ç›˜
    - é˜´çº¿å®šä¹‰ï¼šæ”¶ç›˜ < å¼€ç›˜
    - è¿é˜³å¤©æ•°ï¼šæˆªè‡³å½“æ—¥è¿ç»­é˜³çº¿çš„å¤©æ•°
    - è¿é˜´å¤©æ•°ï¼šæˆªè‡³å½“æ—¥è¿ç»­é˜´çº¿çš„å¤©æ•°
    è¦æ±‚ï¼šæŒ‰è‚¡ç¥¨ï¼ˆä»¥'åç§°'åˆ†ç»„ï¼‰ä¸æ—¥æœŸæ’åºåè®¡ç®—ã€‚

    è¿”å›ï¼šæ–°å¢åˆ— 'è¿é˜³å¤©æ•°', 'è¿é˜´å¤©æ•°', 'é˜³çº¿', 'é˜´çº¿'
    """
    # æ•°æ®éªŒè¯ä¸æ’åº
    required_cols = ['åç§°', 'æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜']
    missing_cols = [c for c in required_cols if c not in df.columns]
    if missing_cols:
        raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—ç”¨äºè¶‹åŠ¿è®¡ç®—: {missing_cols}")

    sorted_df = df.sort(['åç§°', 'æ—¥æœŸ'])

    # æ ‡è®°é˜³çº¿/é˜´çº¿
    sorted_df = sorted_df.with_columns([
        (pl.col('æ”¶ç›˜') > pl.col('å¼€ç›˜')).alias('é˜³çº¿'),
        (pl.col('æ”¶ç›˜') < pl.col('å¼€ç›˜')).alias('é˜´çº¿')
    ])

    # ç”±äºPolarså½“å‰ä¸ç›´æ¥æ”¯æŒæŒ‰æ¡ä»¶çš„é€’å¢è®¡æ•°ï¼Œè¿™é‡Œä½¿ç”¨map_groupsæ‰‹åŠ¨è®¡ç®—
    def _calc_streaks(group: pl.DataFrame) -> pl.DataFrame:
        pos = group['é˜³çº¿'].to_list()
        neg = group['é˜´çº¿'].to_list()
        n = len(pos)
        up_streak = [0] * n
        down_streak = [0] * n
        for i in range(n):
            if pos[i]:
                up_streak[i] = (up_streak[i-1] + 1) if i > 0 else 1
            else:
                up_streak[i] = 0
            if neg[i]:
                down_streak[i] = (down_streak[i-1] + 1) if i > 0 else 1
            else:
                down_streak[i] = 0
        return group.with_columns([
            pl.Series(name='è¿é˜³å¤©æ•°', values=up_streak).cast(pl.Int32),
            pl.Series(name='è¿é˜´å¤©æ•°', values=down_streak).cast(pl.Int32)
        ])

    result = (
        sorted_df
        .group_by('åç§°', maintain_order=True)
        .map_groups(_calc_streaks)
    )

    return result

def compute_limits(df):
    """
    è®¡ç®—æ¶¨è·Œåœä»·æ ¼å’ŒçŠ¶æ€ï¼Œæ ¹æ®è‚¡ç¥¨ä»£ç ç¡®å®šä¸åŒçš„æ¶¨è·Œå¹…é™åˆ¶
    
    å‚æ•°:
    df: polars DataFrameï¼ŒåŒ…å«è‚¡ç¥¨æ•°æ®
    custom_limits: dictï¼Œè‡ªå®šä¹‰æ¶¨è·Œå¹…é™åˆ¶è§„åˆ™ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤è§„åˆ™
    
    è¿”å›:
    ä¿®æ”¹åçš„DataFrameï¼ŒåŒ…å«æ¶¨åœä»·ã€è·Œåœä»·ã€æ¶¨åœã€è·Œåœã€ç‚¸æ¿ç­‰åˆ—
    """
    
    # é»˜è®¤æ¶¨è·Œå¹…é™åˆ¶è§„åˆ™
    custom_limits = {
        '68': 0.20,  # ç§‘åˆ›æ¿20%
        '30': 0.20,  # åˆ›ä¸šæ¿20% 
        '8': 0.30,    # åŒ—äº¤æ‰€30%
        '4': 0.30,    # åŒ—äº¤æ‰€30%
        '9': 0.30,    # åŒ—äº¤æ‰€30%
        'ST': 0.05,   # STè‚¡ç¥¨5%ï¼ˆé€šè¿‡åç§°åˆ¤æ–­ï¼‰
        'default': 0.10  # ä¸»æ¿é»˜è®¤10%
    }

    # æŒ‰åç§°å’Œæ—¥æœŸæ’åº
    df = df.sort(['åç§°', 'æ—¥æœŸ'])
    
    # æ£€æŸ¥æ˜¯å¦æœ‰'æ˜¨æ”¶'åˆ—ï¼Œå¦‚æœæ²¡æœ‰åˆ™è®¡ç®—
    if 'æ˜¨æ”¶' not in df.columns:
        df = df.with_columns([
            pl.col('æ”¶ç›˜').shift(1).over('åç§°').alias('æ˜¨æ”¶')
        ])
    
    # ç¡®ä¿ä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²ç±»å‹
    df = df.with_columns([
        pl.col('ä»£ç ').cast(pl.Utf8).alias('ä»£ç ')
    ])
    
    # æ ¹æ®ä»£ç å’Œåç§°ç¡®å®šæ¶¨è·Œå¹…é™åˆ¶
    def get_limit_pct_expr():
        # ä»é»˜è®¤å€¼å¼€å§‹
        limit_expr = pl.lit(custom_limits.get('default', 0.10))
        
        # å…ˆæ£€æŸ¥STè‚¡ç¥¨ï¼ˆé€šè¿‡åç§°åˆ¤æ–­ï¼‰
        if 'ST' in custom_limits:
            limit_expr = pl.when(
                pl.col('åç§°').str.contains('ST')
            ).then(
                pl.lit(custom_limits['ST'])
            ).otherwise(limit_expr)
        
        # æŒ‰ä»£ç å‰ç¼€åŒ¹é…
        for prefix, limit_pct in custom_limits.items():
            if prefix not in ['ST', 'default']:  # è·³è¿‡ç‰¹æ®Šé”®
                limit_expr = pl.when(
                    pl.col('ä»£ç ').str.starts_with(prefix)
                ).then(
                    pl.lit(limit_pct)
                ).otherwise(limit_expr)
        
        return limit_expr
    
    # æ·»åŠ æ¶¨è·Œå¹…é™åˆ¶åˆ—
    df = df.with_columns([
        get_limit_pct_expr().alias('æ¶¨è·Œå¹…é™åˆ¶')
    ])
    
    # è®¡ç®—æ¶¨åœä»·å’Œè·Œåœä»·
    df = df.with_columns([
        (pl.col('æ˜¨æ”¶') * (1 + pl.col('æ¶¨è·Œå¹…é™åˆ¶'))).round(2).alias('æ¶¨åœä»·'),
        (pl.col('æ˜¨æ”¶') * (1 - pl.col('æ¶¨è·Œå¹…é™åˆ¶'))).round(2).alias('è·Œåœä»·')
    ])
    
    # è®¡ç®—æ¶¨åœã€è·Œåœå’Œç‚¸æ¿çŠ¶æ€
    df = df.with_columns([
        # æ¶¨åœï¼šæ”¶ç›˜æ¶¨è·Œå¹…è¾¾åˆ°æ¶¨è·Œå¹…é™åˆ¶ä¸”æœ€é«˜æ¶¨è·Œå¹…ä¹Ÿè¾¾åˆ°æ¶¨è·Œå¹…é™åˆ¶
        ((pl.col('æ¶¨è·Œå¹…') * 0.01 >= pl.col('æ¶¨è·Œå¹…é™åˆ¶')) |
        (pl.col('æ¶¨åœä»·') == pl.col('æ”¶ç›˜'))).alias('æ¶¨åœ'),
        
        # è·Œåœï¼šæ”¶ç›˜æ¶¨è·Œå¹…è¾¾åˆ°è´Ÿçš„æ¶¨è·Œå¹…é™åˆ¶ä¸”æœ€ä½æ¶¨è·Œå¹…ä¹Ÿè¾¾åˆ°è´Ÿçš„æ¶¨è·Œå¹…é™åˆ¶
        ((pl.col('æ¶¨è·Œå¹…') * 0.01 <= -pl.col('æ¶¨è·Œå¹…é™åˆ¶')) |
        (pl.col('è·Œåœä»·') == pl.col('æ”¶ç›˜'))).alias('è·Œåœ'),
        
        # ç‚¸æ¿ï¼šæœ€é«˜ä»·è§¦åŠæ¶¨åœä»·ä½†æ”¶ç›˜ä»·æœªè¾¾åˆ°æ¶¨åœä»·
        ((pl.col('æœ€é«˜').round(2) == pl.col('æ¶¨åœä»·')) & 
        (pl.col('æ”¶ç›˜').round(2) < pl.col('æ¶¨åœä»·'))).alias('ç‚¸æ¿')
    ])
    
    # ç»Ÿè®¡ä¸åŒæ¶¨è·Œå¹…é™åˆ¶çš„è‚¡ç¥¨æ•°é‡
    limit_stats = df.group_by('æ¶¨è·Œå¹…é™åˆ¶').agg([
        pl.count().alias('æ•°é‡'),
        pl.col('æ¶¨åœ').sum().alias('æ¶¨åœæ•°é‡'),
        pl.col('è·Œåœ').sum().alias('è·Œåœæ•°é‡'),
        pl.col('ç‚¸æ¿').sum().alias('ç‚¸æ¿æ•°é‡')
    ]).sort('æ¶¨è·Œå¹…é™åˆ¶')
    
    print("å„æ¶¨è·Œå¹…é™åˆ¶ç»Ÿè®¡:")
    print(limit_stats.to_pandas())
    
    # æ‰“å°æ€»ä½“è°ƒè¯•ä¿¡æ¯
    total_records = df.height
    total_zhangting = df.filter(pl.col('æ¶¨åœ') == True).height
    total_dieting = df.filter(pl.col('è·Œåœ') == True).height
    total_zhaban = df.filter(pl.col('ç‚¸æ¿') == True).height
    
    print(f"\næ€»è®¡: è®°å½•æ•°={total_records}, æ¶¨åœ={total_zhangting}, è·Œåœ={total_dieting}, ç‚¸æ¿={total_zhaban}")
    
    return df

def calculate_continuous_limit_up_optimized(df):
    """
    è®¡ç®—è¿æ¿æ•°å’Œè¿æ¿å¤©æ•°
    è¿æ¿æ•°ï¼šè¿ç»­æ¶¨åœçš„å¤©æ•°ï¼ˆå¦‚6å¤©6æ¿ï¼‰
    è¿æ¿å¤©æ•°ï¼šæŒ‡å®šæ—¶é—´çª—å£å†…çš„æ¶¨åœå¤©æ•°ï¼ˆå¦‚7å¤©5æ¿ï¼‰
    """
    # ç¡®ä¿æ•°æ®æŒ‰ä»£ç å’Œæ—¥æœŸæ’åº
    df = df.sort(['åç§°', 'æ—¥æœŸ'])
    
    # åˆ›å»ºæ¶¨åœåºåˆ—åˆ†ç»„æ ‡è¯† - ä¿®å¤cumsumè°ƒç”¨
    df = (
        df
        .with_columns([
            (
                (pl.col('æ¶¨åœ') != pl.col('æ¶¨åœ').shift(1, fill_value=False).over('åç§°'))
                .cast(pl.Int32)
                .alias('is_limit_changed')
            )
        ])
        .lazy()
        .with_columns([
            pl.col('is_limit_changed').cumsum().over('åç§°').alias('limit_group')
        ])
        .collect()
    )

    # è®¡ç®—è¿æ¿æ•°ï¼ˆè¿ç»­æ¶¨åœå¤©æ•°ï¼‰
    df = df.with_columns([
        pl.when(pl.col('æ¶¨åœ'))
        .then(
            (pl.int_range(0, pl.count()).over(['åç§°', 'limit_group']) + 1)
        )
        .otherwise(0)
        .cast(pl.Int32)
        .alias('è¿æ¿æ•°')
    ])
        # æ­¥éª¤1ï¼šé«˜æ•ˆçš„é¢„æ£€æŸ¥ - è®¡ç®—å‰5å¤©æ¶¨åœç´¯è®¡æ•°
    df = df.with_columns([
        pl.col('æ¶¨åœ').cast(pl.Int32).alias('limit_up'),
        # è®¡ç®—åŒ…å«ä»Šå¤©åœ¨å†…çš„å‰5å¤©æ¶¨åœæ¬¡æ•°
        pl.col('æ¶¨åœ').cast(pl.Int32)
        .rolling_sum(window_size=5, min_periods=1)
        .over('åç§°')
        .alias('last_5days_count')
    ])
    
    # æ­¥éª¤2ï¼šåªå¯¹éœ€è¦è¯¦ç»†è®¡ç®—çš„è¡Œè¿›è¡Œå¤„ç†
    def process_group_with_precheck(group_df):
        result = []
        n = len(group_df)
        
        for i in range(n):
            current_limit_up = group_df['limit_up'][i]
            last_5days_sum = group_df['last_5days_count'][i]
            
            # å¦‚æœä»Šå¤©ä¸æ¶¨åœï¼Œç›´æ¥æ ‡è®°ä¸º0å¤©0æ¿
            if current_limit_up != 1:
                result.append("0å¤©0æ¿")
                continue
            
            # å¦‚æœå‰5å¤©(å«ä»Šå¤©)æ¶¨åœæ¬¡æ•°<=1ï¼Œè¯´æ˜åªæœ‰ä»Šå¤©æ¶¨åœ
            if last_5days_sum <= 1:
                result.append("1å¤©1æ¿")
                continue
            
            # éœ€è¦è¯¦ç»†å›æº¯è®¡ç®—
            consecutive_count = 1
            start_pos = i
            
            # ä»å‰å¾€åé€ä¸ªéå†ï¼Œæ‰¾åˆ°æœ€æ—©çš„æ¶¨åœ
            current_pos = i - 1
            while current_pos >= 0:
                # åœ¨5å¤©çª—å£å†…æŸ¥æ‰¾æ¶¨åœ
                found_in_window = False
                window_start = max(0, current_pos - 4)  # 5å¤©çª—å£çš„èµ·å§‹ä½ç½®
                
                # ä»å½“å‰ä½ç½®å‘å‰æœç´¢5å¤©
                for j in range(current_pos, window_start - 1, -1):
                    if group_df['limit_up'][j] == 1:
                        consecutive_count += 1
                        start_pos = j
                        current_pos = j - 1  # ä»æ‰¾åˆ°çš„æ¶¨åœä½ç½®ç»§ç»­å‘å‰
                        found_in_window = True
                        break
                
                # å¦‚æœ5å¤©å†…æ²¡æ‰¾åˆ°æ¶¨åœï¼Œåœæ­¢å›æº¯
                if not found_in_window:
                    break
            
            # è®¡ç®—æ€»å¤©æ•°ï¼ˆä»æœ€æ—©æ¶¨åœåˆ°ä»Šå¤©ï¼‰
            total_days = i - start_pos + 1
            result.append(f"{total_days}å¤©{consecutive_count}æ¿")
        
        return pl.DataFrame({'è¿æ¿å¤©æ•°': result})
    
    # åº”ç”¨åˆ°æ¯ä¸ªè‚¡ç¥¨åˆ†ç»„
    result_df = df.group_by('åç§°', maintain_order=True).map_groups(process_group_with_precheck)
    
    # åˆå¹¶ç»“æœ
    df = df.with_columns([
        result_df.get_column('è¿æ¿å¤©æ•°')
    ])
    
    # æ¸…ç†ä¸´æ—¶åˆ—
    df = df.drop(['limit_group', 'limit_up', 'last_5days_count','is_limit_changed'])
    
    return df

def create_limit_status_parquet(df, output_path):
    """åˆ›å»ºæ¶¨åœè·ŒåœçŠ¶æ€parquetæ–‡ä»¶"""
    # è®¡ç®—æ¶¨è·Œåœã€è¿æ¿ã€æŠ€æœ¯æŒ‡æ ‡ä¸ç›¸å¯¹æŒ‡æ ‡ï¼Œç¡®ä¿åŒ…å«5/10/20æ—¥æ¶¨è·Œå¹…ä¸MAåˆ—
    status_df = compute_limits(df)

    # ç¡®ä¿æ¶¨åœã€è·Œåœå’Œç‚¸æ¿åˆ—æ˜¯å¸ƒå°”ç±»å‹
    status_df = status_df.with_columns([
        pl.col('æ¶¨åœ').cast(pl.Boolean).alias('æ¶¨åœ'),
        pl.col('è·Œåœ').cast(pl.Boolean).alias('è·Œåœ'),
        pl.col('ç‚¸æ¿').cast(pl.Boolean).alias('ç‚¸æ¿')
    ])

    # åˆå§‹åŒ–å¹¶è®¡ç®—è¿æ¿å¤©æ•°/è¿æ¿æ•°
    status_df = status_df.with_columns([
        pl.lit(0).cast(pl.Int32).alias('è¿æ¿å¤©æ•°'),
        pl.lit(0).cast(pl.Int32).alias('è¿æ¿æ•°')
    ])
    status_df = calculate_continuous_limit_up_optimized(status_df)

    # è®¡ç®—è‚¡ç¥¨æ¶¨è·Œå¹…ç›¸å…³æŠ€æœ¯æŒ‡æ ‡ï¼ˆ5/10/20æ—¥æ¶¨è·Œå¹…ä¸MA5/MA10/MA20ï¼‰
    try:
        status_df = calculate_stock_indicators(status_df)
        # ç›¸å¯¹å‡çº¿ç±»æŒ‡æ ‡ï¼ˆå¯é€‰ï¼‰
        status_df = add_price_relative_indicators(status_df)
    except Exception as _e:
        print(f"è®¡ç®—æŠ€æœ¯æŒ‡æ ‡æ—¶å‡ºç°é—®é¢˜ï¼Œå°†ç»§ç»­ä¿å­˜åŸºç¡€çŠ¶æ€æ•°æ®: {_e}")

    # æ·»åŠ Kçº¿è¶‹åŠ¿æŒ‡æ ‡ï¼šé˜³çº¿/é˜´çº¿ä¸è¿é˜³å¤©æ•°/è¿é˜´å¤©æ•°
    try:
        status_df = add_candlestick_trend_streaks(status_df)
    except Exception as _e:
        print(f"è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡æ—¶å‡ºç°é—®é¢˜ï¼Œå°†ç»§ç»­ä¿å­˜: {_e}")
    
    # ä¸è¿‡æ»¤ä»»ä½•è®°å½•ï¼Œä¿å­˜æ‰€æœ‰è®°å½•ï¼ˆåŒ…æ‹¬æ¶¨åœå’Œè·Œåœï¼‰
    print(f"æ€»è®°å½•æ•°: {status_df.height}, æ¶¨åœè®°å½•æ•°: {status_df.filter(pl.col('æ¶¨åœ') == True).height}, è·Œåœè®°å½•æ•°: {status_df.filter(pl.col('è·Œåœ') == True).height}")
    
    # å®‰å…¨å†™å…¥parquetæ–‡ä»¶
    if not safe_write_parquet(status_df, output_path):
        raise Exception(f"å†™å…¥å¸‚åœºçŠ¶æ€æ•°æ®æ–‡ä»¶å¤±è´¥: {output_path}")
    
    return status_df

def safe_write_parquet(df: pl.DataFrame, file_path: str, max_retries: int = 3) -> bool:
    """
    å®‰å…¨å†™å…¥parquetæ–‡ä»¶ï¼Œæ”¯æŒé‡è¯•å’Œæ–‡ä»¶é”
    
    Args:
        df: è¦å†™å…¥çš„DataFrame
        file_path: æ–‡ä»¶è·¯å¾„
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
    
    Returns:
        bool: æ˜¯å¦å†™å…¥æˆåŠŸ
    """
    if df is None or df.is_empty():
        print(f"âš ï¸ DataFrameä¸ºç©ºï¼Œè·³è¿‡å†™å…¥: {file_path}")
        return False
    
    file_path = str(file_path)
    
    for attempt in range(max_retries):
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(file_path), exist_ok=True)
            
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶å†™å…¥ï¼Œç„¶ååŸå­æ€§ç§»åŠ¨
            temp_file = None
            with tempfile.NamedTemporaryFile(
                mode='wb', 
                delete=False, 
                dir=os.path.dirname(file_path),
                suffix='.tmp'
            ) as temp_file:
                temp_path = temp_file.name
            
            # å†™å…¥ä¸´æ—¶æ–‡ä»¶
            df.write_parquet(temp_path)
            
            # åŸå­æ€§ç§»åŠ¨åˆ°ç›®æ ‡ä½ç½®
            shutil.move(temp_path, file_path)
            
            print(f"âœ… æˆåŠŸå†™å…¥æ–‡ä»¶: {file_path} ({df.height} è¡Œ)")
            return True
            
        except Exception as e:
            print(f"âŒ å†™å…¥æ–‡ä»¶å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file and os.path.exists(temp_file.name):
                try:
                    os.unlink(temp_file.name)
                except:
                    pass
            
            if attempt < max_retries - 1:
                time.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
            else:
                return False
    
    return False


class MarketMetadataManager:
    """å¸‚åœºå…ƒæ•°æ®ç®¡ç†ç±»ï¼Œå¤„ç†å¸‚åœºæƒ…ç»ªæŒ‡æ ‡ï¼Œå¦‚çº¢ç›˜ç‡ã€æ¶¨åœæ•°ã€è·Œåœæ•°ã€åœ°å¤©æ¿ä¸ªæ•°ç­‰"""

    def __init__(self, metadata_path: str = None,
                 stock_metadata_path: str = None,
                 market_states_path: str = None):
        if metadata_path is None:
            # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•çš„data_cache/otherç›®å½•ï¼ˆå®é™…æ•°æ®å­˜å‚¨ä½ç½®ï¼‰
            self.metadata_path = Path("data_cache/other/market_metadata.parquet")
        else:
            self.metadata_path = Path(metadata_path)

        if stock_metadata_path is None:
            self.stock_metadata_path = Path("data_cache/stock_daily/stock_daily_metadata.parquet")
        else:
            self.stock_metadata_path = Path(stock_metadata_path)

        if market_states_path is None:
            self.market_states_path = Path("data_cache/other/market_states.parquet")
        else:
            self.market_states_path = Path(market_states_path)

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.metadata_path.parent.mkdir(parents=True, exist_ok=True)
        self.stock_metadata_path.parent.mkdir(parents=True, exist_ok=True)
        self.market_states_path.parent.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–ç¼“å­˜å±æ€§
        self._market_states_cache = None
        self._metadata_cache = None
        self._cache_timestamp = None

        print(f"ğŸ“Š å¸‚åœºå…ƒæ•°æ®ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    def clear_cache(self):
        """æ¸…ç†å†…å­˜ç¼“å­˜"""
        self._market_states_cache = None
        self._metadata_cache = None
        self._cache_timestamp = None
        print("MarketMetadataManager å†…å­˜ç¼“å­˜å·²æ¸…ç†")
    
    def is_latest_trading_day(self) -> bool:
        """æ£€æŸ¥å¸‚åœºå…ƒæ•°æ®æ˜¯å¦æ˜¯æœ€æ–°äº¤æ˜“æ—¥çš„æ•°æ®

        é€»è¾‘ï¼š
        1. è·å–ç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
        2. è·å–å½“å‰åº”è¯¥æ›´æ–°åˆ°çš„æœ€æ–°äº¤æ˜“æ—¥æœŸ
        3. åˆ¤æ–­å½“å¤©æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼Œæ˜¯å¦å·²è¿‡18:00
        4. è€ƒè™‘å‘¨æœ«å’ŒèŠ‚å‡æ—¥çš„å½±å“
        """
        try:
            # 1. è·å–ç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            metadata = self.load_metadata()
            if metadata is None or metadata.is_empty():
                print("å¸‚åœºå…ƒæ•°æ®ä¸ºç©ºï¼Œéœ€è¦æ›´æ–°")
                return False

            if 'æ—¥æœŸ' not in metadata.columns:
                print("è­¦å‘Š: å¸‚åœºå…ƒæ•°æ®ä¸­ç¼ºå°‘æ—¥æœŸåˆ—")
                return False

            # è§£æç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸ
            latest_date_raw = metadata['æ—¥æœŸ'].max()
            if isinstance(latest_date_raw, str):
                try:
                    latest_local_date = datetime.strptime(latest_date_raw, '%Y-%m-%d').date()
                except ValueError:
                    try:
                        latest_local_date = datetime.strptime(latest_date_raw, '%Y/%m/%d').date()
                    except ValueError:
                        latest_local_date = datetime.strptime(latest_date_raw, '%Y%m%d').date()
            elif isinstance(latest_date_raw, datetime):
                latest_local_date = latest_date_raw.date()
            elif isinstance(latest_date_raw, date):
                latest_local_date = latest_date_raw
            else:
                print(f"âš ï¸ æœªçŸ¥çš„æ—¥æœŸç±»å‹: {type(latest_date_raw)}, å€¼: {latest_date_raw}")
                return False

            # 2. è·å–å½“å‰æ—¶é—´ä¿¡æ¯
            now = datetime.now()
            current_date = now.date()
            current_time = now.time()

            # 3. å®šä¹‰æ•°æ®æ›´æ–°æ—¶é—´ï¼ˆ18:00åè®¤ä¸ºå½“æ—¥æ•°æ®å·²æ›´æ–°ï¼‰
            from datetime import time as dt_time
            update_time = dt_time(18, 0)  # 18:00

            # 4. ä½¿ç”¨holidaysåº“åˆ¤æ–­èŠ‚å‡æ—¥
            def is_holiday(check_date):
                """ä½¿ç”¨holidaysåº“åˆ¤æ–­æ˜¯å¦ä¸ºä¸­å›½èŠ‚å‡æ—¥"""
                try:
                    import holidays
                    # åˆ›å»ºä¸­å›½èŠ‚å‡æ—¥å¯¹è±¡
                    china_holidays = holidays.China(years=check_date.year)
                    return check_date in china_holidays
                except Exception as e:
                    print(f"âš ï¸ èŠ‚å‡æ—¥åˆ¤æ–­å¤±è´¥: {e}")
                    return False

            # 5. åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥ï¼ˆéå‘¨æœ«ä¸”éèŠ‚å‡æ—¥ï¼‰
            def is_trading_day(check_date):
                """åˆ¤æ–­æ˜¯å¦ä¸ºäº¤æ˜“æ—¥"""
                # å‘¨æœ«ä¸æ˜¯äº¤æ˜“æ—¥
                if check_date.weekday() >= 5:  # 5=å‘¨å…­, 6=å‘¨æ—¥
                    return False
                # èŠ‚å‡æ—¥ä¸æ˜¯äº¤æ˜“æ—¥
                if is_holiday(check_date):
                    return False
                return True

            # 6. è·å–å‰ä¸€ä¸ªäº¤æ˜“æ—¥
            def get_previous_trading_day(from_date):
                """è·å–æŒ‡å®šæ—¥æœŸå‰çš„æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥"""
                check_date = from_date - timedelta(days=1)
                for _ in range(15):  # æœ€å¤šå¾€å‰æ‰¾15å¤©ï¼ˆè€ƒè™‘é•¿å‡æœŸï¼‰
                    if is_trading_day(check_date):
                        return check_date
                    check_date -= timedelta(days=1)
                # å¦‚æœ15å¤©å†…éƒ½æ²¡æ‰¾åˆ°ï¼Œè¿”å›15å¤©å‰çš„æ—¥æœŸ
                return from_date - timedelta(days=15)

            # 7. è·å–æœ€æ–°åº”è¯¥æ›´æ–°åˆ°çš„äº¤æ˜“æ—¥æœŸ
            def get_latest_expected_trading_date():
                """è·å–æœ€æ–°åº”è¯¥æ›´æ–°åˆ°çš„äº¤æ˜“æ—¥æœŸ"""
                if is_trading_day(current_date):
                    # ä»Šå¤©æ˜¯äº¤æ˜“æ—¥
                    if current_time >= update_time:
                        # å·²è¿‡18:00ï¼Œåº”è¯¥æœ‰ä»Šå¤©çš„æ•°æ®
                        return current_date
                    else:
                        # æœªè¿‡18:00ï¼Œåº”è¯¥æœ‰å‰ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
                        return get_previous_trading_day(current_date)
                else:
                    # ä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œåº”è¯¥æœ‰æœ€è¿‘ä¸€ä¸ªäº¤æ˜“æ—¥çš„æ•°æ®
                    return get_previous_trading_day(current_date)

            # 8. æ¯”è¾ƒç°æœ‰æ•°æ®çš„æœ€æ–°æ—¥æœŸä¸æœ€æ–°äº¤æ˜“æ—¥ï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦æ›´æ–°
            expected_latest_date = get_latest_expected_trading_date()

            print(f"ğŸ“Š ç°æœ‰æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_local_date}")
            print(f"ğŸ“Š æœ€æ–°äº¤æ˜“æ—¥æœŸ: {expected_latest_date}")

            # å¦‚æœç°æœ‰æ•°æ®æ—¥æœŸ >= æœ€æ–°äº¤æ˜“æ—¥æœŸï¼Œåˆ™è®¤ä¸ºæ˜¯æœ€æ–°çš„
            is_latest = latest_local_date >= expected_latest_date

            if is_latest:
                print("âœ… å¸‚åœºå…ƒæ•°æ®å·²æ˜¯æœ€æ–°ï¼Œæ— éœ€æ›´æ–°")
            else:
                print("ğŸ“Š å¸‚åœºå…ƒæ•°æ®éœ€è¦æ›´æ–°")

            return is_latest

        except Exception as e:
            print(f"âŒ æ£€æŸ¥æ˜¯å¦ä¸ºæœ€æ–°äº¤æ˜“æ—¥å¤±è´¥: {e}")
            return False
    
    def precompute_market_states(self):
        """é¢„è®¡ç®—å¸‚åœºçŠ¶æ€æ•°æ®ï¼ŒåŒ…æ‹¬æ¶¨åœã€è·Œåœã€ç‚¸æ¿ã€è¿æ¿é«˜åº¦ç­‰"""
        try:
            print("å¼€å§‹é¢„è®¡ç®—å¸‚åœºçŠ¶æ€æ•°æ®...")
            
            # åŠ è½½è‚¡ç¥¨æ—¥Kå…ƒæ•°æ®
            stock_data = self.load_stock_metadata()
            if stock_data is None or stock_data.is_empty():
                print("è‚¡ç¥¨æ—¥Kå…ƒæ•°æ®ä¸ºç©ºï¼Œæ— æ³•é¢„è®¡ç®—å¸‚åœºçŠ¶æ€")
                return False
            
            # åˆ›å»ºæ¶¨åœè·ŒåœçŠ¶æ€æ–‡ä»¶
            create_limit_status_parquet(stock_data, self.market_states_path)
            
            print("å¸‚åœºçŠ¶æ€æ•°æ®é¢„è®¡ç®—å®Œæˆ")
            return True
        except Exception as e:
            print(f"é¢„è®¡ç®—å¸‚åœºçŠ¶æ€æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False
    
    def update_market_states_incremental(self):
        """å¢é‡æ›´æ–°å¸‚åœºçŠ¶æ€æ•°æ®"""
        try:
            if os.path.exists(self.market_states_path):
                # åŠ è½½ç°æœ‰çŠ¶æ€æ•°æ®
                existing_states = pl.read_parquet(self.market_states_path)
                latest_date = existing_states['æ—¥æœŸ'].max()
                print(f"å¸‚åœºçŠ¶æ€æ•°æ®æœ€æ–°æ—¥æœŸ: {latest_date}")
                # åŠ è½½è‚¡ç¥¨å…ƒæ•°æ®
                stock_data = self.load_stock_metadata()
                # è®¡ç®—æ–°æ•°æ®çš„çŠ¶æ€
                stock_data = compute_limits(stock_data)
                
                # å¯¹äºè¿æ¿å¤©æ•°è®¡ç®—ï¼Œéœ€è¦è·å–å†å²æ•°æ®
                # è·å–æ¯ä¸ªè‚¡ç¥¨åœ¨æœ€æ–°æ—¥æœŸçš„è¿æ¿çŠ¶æ€
                stock_codes = stock_data['åç§°'].unique()
                
                # è®¡ç®—è¿æ¿é«˜åº¦
                stock_data = calculate_continuous_limit_up_optimized(stock_data)
                stock_data = calculate_stock_indicators(stock_data)
    
                stock_data = add_price_relative_indicators(stock_data)

                # å¢é‡æµç¨‹ä¹Ÿè¡¥å……è¶‹åŠ¿æŒ‡æ ‡
                try:
                    stock_data = add_candlestick_trend_streaks(stock_data)
                except Exception as _e:
                    print(f"å¢é‡æ›´æ–°è®¡ç®—è¶‹åŠ¿æŒ‡æ ‡å¤±è´¥ï¼ˆå¿½ç•¥ï¼‰: {_e}")

                stock_data = stock_data.unique(subset=['æ—¥æœŸ', 'åç§°'])
                # å®‰å…¨ä¿å­˜æ›´æ–°åçš„çŠ¶æ€æ•°æ®
                if safe_write_parquet(stock_data, self.market_states_path):
                    print("âœ… å¸‚åœºçŠ¶æ€æ•°æ®å¢é‡æ›´æ–°æˆåŠŸ")
                    return True
                else:
                    print("âŒ å¸‚åœºçŠ¶æ€æ•°æ®å¢é‡æ›´æ–°å¤±è´¥")
                    return False
            else:
                # å¦‚æœä¸å­˜åœ¨çŠ¶æ€æ–‡ä»¶ï¼Œåˆ™è¿›è¡Œå…¨é‡è®¡ç®—
                return self.precompute_market_states()
        except Exception as e:
            print(f"å¢é‡æ›´æ–°å¸‚åœºçŠ¶æ€æ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            return False

    def calculate_daily_market_stats_optimized(self, stock_data, date_val):
        """ä¼˜åŒ–çš„å¸‚åœºç»Ÿè®¡æŒ‡æ ‡è®¡ç®—æ–¹æ³•"""
        # ç»Ÿä¸€æ—¥æœŸåˆ—ä¸å…¥å‚ç±»å‹
        stock_data = _ensure_date_column(stock_data, 'æ—¥æœŸ')
        date_obj = _parse_to_date(date_val)
        # ç­›é€‰å½“å¤©çš„æ•°æ®ï¼ˆä½¿ç”¨ Date ç±»å‹æ¯”è¾ƒï¼‰
        day_data = stock_data.filter(pl.col('æ—¥æœŸ') == pl.lit(date_obj))
        
        if day_data.is_empty():
            return self._empty_stats(date_val)
        
        # ä¸€æ¬¡æ€§è®¡ç®—æ‰€æœ‰åŸºæœ¬æŒ‡æ ‡
        counts = day_data.select([
            pl.count().alias('æ€»è‚¡ç¥¨æ•°'),
            pl.sum(pl.col('æ¶¨è·Œå¹…') > 0).alias('ä¸Šæ¶¨è‚¡ç¥¨æ•°'),
            pl.sum(pl.col('æ¶¨è·Œå¹…') < 0).alias('ä¸‹è·Œè‚¡ç¥¨æ•°'),
            pl.sum(pl.col('æ¶¨è·Œå¹…') == 0).alias('å¹³ç›˜è‚¡ç¥¨æ•°'),
            pl.sum(pl.col('æ¶¨åœ') == True).alias('æ¶¨åœæ•°'),
            pl.sum(pl.col('è·Œåœ') == True).alias('è·Œåœæ•°'),
            pl.sum(pl.col('ç‚¸æ¿') == True).alias('ç‚¸æ¿æ•°'),
            pl.sum(pl.col('æˆäº¤é¢')).alias('æˆäº¤æ€»é¢')
        ]).to_dicts()[0]
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"ä¼˜åŒ–æ–¹æ³•è®¡ç®—ç»“æœ - æ€»è‚¡ç¥¨æ•°: {counts['æ€»è‚¡ç¥¨æ•°']}, æ¶¨åœæ•°: {counts['æ¶¨åœæ•°']}, è·Œåœæ•°: {counts['è·Œåœæ•°']}, ç‚¸æ¿æ•°: {counts['ç‚¸æ¿æ•°']}")
        
        # è®¡ç®—çº¢ç›˜ç‡
        counts['çº¢ç›˜ç‡'] = (counts['ä¸Šæ¶¨è‚¡ç¥¨æ•°'] / counts['æ€»è‚¡ç¥¨æ•°'] * 100) if counts['æ€»è‚¡ç¥¨æ•°'] > 0 else 0
        
        # è®¡ç®—åœ°å¤©æ¿ä¸ªæ•° - æœ€ä½ä¸ºè·Œåœä»·ï¼ŒåŒæ—¶æ¶¨åœ
        ground_ceiling_count = day_data.filter(
            (pl.col('æœ€ä½') == pl.col('è·Œåœä»·')) & 
            (pl.col('æ”¶ç›˜') == pl.col('æ¶¨åœä»·'))
        ).height
        counts['åœ°å¤©æ¿æ•°'] = ground_ceiling_count
        
        # è®¡ç®—è¿æ¿é«˜åº¦åˆ†å¸ƒ
        if 'è¿æ¿å¤©æ•°' in day_data.columns:
            # ç›´æ¥ä»é¢„è®¡ç®—çš„åˆ—è·å–è¿æ¿é«˜åº¦åˆ†å¸ƒ
            continuous_stats = day_data.filter(pl.col('è¿æ¿å¤©æ•°') > 0).select([
                pl.col('è¿æ¿å¤©æ•°').value_counts()
            ]).to_dicts()[0]
            
            # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼
            for i in range(1, 6):
                counts[f'{i}è¿æ¿æ•°'] = continuous_stats.get(i, 0)
            counts['6è¿æ¿æ•°'] = sum(v for k, v in continuous_stats.items() if k >= 6)
        else:
            # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„è¿æ¿å¤©æ•°ï¼Œä½¿ç”¨ä¼˜åŒ–çš„æ–¹æ³•è®¡ç®—
            continuous_limit_up_count = self._calculate_continuous_limit_up_optimized(
                stock_data, date_val
            )
            
            # ç»Ÿè®¡è¿æ¿é«˜åº¦åˆ†å¸ƒ
            for i in range(1, 6):
                counts[f'{i}è¿æ¿æ•°'] = sum(1 for v in continuous_limit_up_count.values() if v == i)
            counts['6è¿æ¿æ•°'] = sum(1 for v in continuous_limit_up_count.values() if v >= 6)
        
        # æ·»åŠ æ—¥æœŸ
        counts['æ—¥æœŸ'] = date_obj
        
        # è½¬æ¢æˆäº¤é¢å•ä½ä¸ºäº¿å…ƒ
        counts['æˆäº¤æ€»é¢'] = counts['æˆäº¤æ€»é¢'] / 100000000
        
        return counts
    
    def _empty_stats(self, date_val):
        """è¿”å›ç©ºçš„ç»Ÿè®¡ç»“æœ"""
        return {
            'æ—¥æœŸ': date_val,
            'çº¢ç›˜ç‡': 0.0,
            'æ¶¨åœæ•°': 0,
            'è·Œåœæ•°': 0,
            'ç‚¸æ¿æ•°': 0,
            'åœ°å¤©æ¿æ•°': 0,
            'æ€»è‚¡ç¥¨æ•°': 0,
            'ä¸Šæ¶¨è‚¡ç¥¨æ•°': 0,
            'æˆäº¤æ€»é¢': 0.0,
            '1è¿æ¿æ•°': 0,
            '2è¿æ¿æ•°': 0,
            '3è¿æ¿æ•°': 0,
            '4è¿æ¿æ•°': 0,
            '5è¿æ¿æ•°': 0,
            '6è¿æ¿æ•°': 0,
            'æœ€é«˜è¿æ¿æ•°': 0,
            'æœ€é«˜è¿æ¿è‚¡ç¥¨æ•°': 0,
            'æœ€é«˜è¿æ¿è‚¡ç¥¨åç§°': '',
            'è¿æ¿æ€»æ•°': 0
        }
    
    def _calculate_continuous_limit_up_optimized(self, stock_data, date_val):
        """ä¼˜åŒ–çš„è¿æ¿é«˜åº¦è®¡ç®—æ–¹æ³•"""
        # è·å–å½“å‰æ—¥æœŸå‰30å¤©çš„æ•°æ®
        if isinstance(date_val, date):
            end_date = date_val
            start_date = end_date - timedelta(days=30)
        else:
            end_date = datetime.strptime(date_val, '%Y-%m-%d').date()
            start_date = end_date - timedelta(days=30)
        
        # ç­›é€‰æ—¥æœŸèŒƒå›´å†…çš„æ•°æ®ï¼ˆä½¿ç”¨æ—¥æœŸå¯¹è±¡è€Œéå­—ç¬¦ä¸²ï¼‰
        period_data = stock_data.filter(
            (pl.col('æ—¥æœŸ') >= pl.lit(start_date)) & 
            (pl.col('æ—¥æœŸ') <= pl.lit(end_date))
        )
        
        # è®¡ç®—è¿æ¿é«˜åº¦
        period_data = calculate_continuous_limit_up_optimized(period_data)
        
        # ç­›é€‰å½“å¤©çš„æ•°æ®ï¼ˆä½¿ç”¨æ—¥æœŸå¯¹è±¡ï¼‰
        day_data = period_data.filter(pl.col('æ—¥æœŸ') == pl.lit(end_date))
        
        # æå–è¿æ¿é«˜åº¦ç»“æœ
        result = {}
        for row in day_data.filter(pl.col('è¿æ¿å¤©æ•°') > 0).select(['åç§°', 'è¿æ¿å¤©æ•°']).to_dicts():
            result[row['åç§°']] = row['è¿æ¿å¤©æ•°']
        
        return result
    
    def load_metadata(self) -> Optional[pl.DataFrame]:
        """åŠ è½½å¸‚åœºå…ƒæ•°æ®æ–‡ä»¶"""
        if not os.path.exists(self.metadata_path):
            print(f"å¸‚åœºå…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.metadata_path}")
            return None
        df = pl.read_parquet(self.metadata_path)
        # ç»Ÿä¸€æ—¥æœŸåˆ—ä¸º Date ç±»å‹
        df = _ensure_date_column(df, 'æ—¥æœŸ')
        return df

    def get_latest_daily_trade_date(self) -> Optional[date]:
        """è·å–å¸‚åœºå…ƒæ•°æ®ä¸­çš„æœ€æ–°äº¤æ˜“æ—¥æœŸ"""
        try:
            df = self.load_metadata()
            if df is not None and not df.is_empty():
                latest_date = df['æ—¥æœŸ'].max()
                if isinstance(latest_date, str):
                    return datetime.strptime(latest_date, '%Y-%m-%d').date()
                elif isinstance(latest_date, datetime):
                    return latest_date.date()
                elif isinstance(latest_date, date):
                    return latest_date
            return None
        except Exception as e:
            print(f"è·å–æœ€æ–°äº¤æ˜“æ—¥æœŸå¤±è´¥: {e}")
            return None

    def load_stock_metadata(self) -> Optional[pl.DataFrame]:
        """åŠ è½½è‚¡ç¥¨æ—¥Kå…ƒæ•°æ®æ–‡ä»¶"""
        if not os.path.exists(self.stock_metadata_path):
            print(f"è‚¡ç¥¨æ—¥Kå…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.stock_metadata_path}")
            return None
            
        try:
            df = pl.read_parquet(self.stock_metadata_path)
            df = _ensure_date_column(df, 'æ—¥æœŸ')
            return df
        except Exception as e:
            print(f"è¯»å–è‚¡ç¥¨æ—¥Kå…ƒæ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
            return None
    
    def load_market_states(self) -> Optional[pl.DataFrame]:
        """åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®æ–‡ä»¶ï¼Œå¸¦æ™ºèƒ½ç¼“å­˜å’Œæ–‡ä»¶ä¿®å¤"""
        # æ£€æŸ¥å†…å­˜ç¼“å­˜æ˜¯å¦æœ‰æ•ˆï¼ˆ5åˆ†é’Ÿå†…ï¼‰
        if (self._market_states_cache is not None and
            self._cache_timestamp is not None and
            (datetime.now() - self._cache_timestamp).seconds < 300):
            print("ä½¿ç”¨å†…å­˜ç¼“å­˜çš„å¸‚åœºçŠ¶æ€æ•°æ®")
            return self._market_states_cache

        if not os.path.exists(self.market_states_path):
            print(f"å¸‚åœºçŠ¶æ€æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {self.market_states_path}")
            # å°è¯•é¢„è®¡ç®—ç”Ÿæˆ
            try:
                print("å°è¯•é¢„è®¡ç®—ç”Ÿæˆå¸‚åœºçŠ¶æ€æ•°æ®...")
                if self.precompute_market_states():
                    print("âœ… é¢„è®¡ç®—æˆåŠŸï¼Œé‡æ–°åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®")
                    return self.load_market_states()
                else:
                    print("âŒ é¢„è®¡ç®—å¤±è´¥ï¼Œæ— æ³•åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®")
                    return None
            except Exception as gen_e:
                print(f"é¢„è®¡ç®—å¸‚åœºçŠ¶æ€æ•°æ®å¤±è´¥: {str(gen_e)}")
                return None

        try:
            print("ä»æ–‡ä»¶åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®")
            data = pl.read_parquet(self.market_states_path)

            # ç¡®ä¿æ—¥æœŸåˆ—ä¸º Date ç±»å‹
            data = _ensure_date_column(data, 'æ—¥æœŸ')

            # ç¡®ä¿è‚¡ç¥¨ä»£ç ä¸º6ä½æ•°å­—ï¼ˆ0å¡«å……ï¼‰
            if 'ä»£ç ' in data.columns:
                data = data.with_columns([
                    pl.col('ä»£ç ').cast(pl.Utf8).str.zfill(6).alias('ä»£ç ')
                ])

            # æ›´æ–°å†…å­˜ç¼“å­˜
            self._market_states_cache = data
            self._cache_timestamp = datetime.now()
            return data
            
        except Exception as e:
            print(f"è¯»å–å¸‚åœºçŠ¶æ€æ•°æ®æ–‡ä»¶å¤±è´¥: {str(e)}")
            
            # å°è¯•ä¿®å¤æŸåçš„æ–‡ä»¶
            if "Invalid thrift" in str(e) or "File out of specification" in str(e):
                print("æ£€æµ‹åˆ°parquetæ–‡ä»¶æŸåï¼Œå°è¯•ä¿®å¤...")
                try:
                    # å¤‡ä»½æŸåçš„æ–‡ä»¶
                    backup_path = f"{self.market_states_path}.corrupted_{int(time.time())}"
                    shutil.move(self.market_states_path, backup_path)
                    print(f"å·²å¤‡ä»½æŸåæ–‡ä»¶åˆ°: {backup_path}")
                    
                    # å°è¯•é‡æ–°ç”Ÿæˆå¸‚åœºçŠ¶æ€æ•°æ®
                    print("å°è¯•é‡æ–°ç”Ÿæˆå¸‚åœºçŠ¶æ€æ•°æ®...")
                    if self.precompute_market_states():
                        print("âœ… å¸‚åœºçŠ¶æ€æ•°æ®é‡æ–°ç”ŸæˆæˆåŠŸ")
                        # é‡æ–°åŠ è½½
                        return self.load_market_states()
                    else:
                        print("âŒ å¸‚åœºçŠ¶æ€æ•°æ®é‡æ–°ç”Ÿæˆå¤±è´¥")
                        return None
                        
                except Exception as repair_error:
                    print(f"ä¿®å¤æ–‡ä»¶å¤±è´¥: {str(repair_error)}")
                    return None
            
            return None
    
    def get_market_data_by_date(self, date_val: Union[str, date]) -> Optional[pl.DataFrame]:
        """è·å–æŒ‡å®šæ—¥æœŸçš„å¸‚åœºæ•°æ®
        
        Args:
            date_val: æ—¥æœŸï¼Œå¯ä»¥æ˜¯å­—ç¬¦ä¸²æ ¼å¼('YYYY-MM-DD')æˆ–dateå¯¹è±¡
            
        Returns:
            æŒ‡å®šæ—¥æœŸçš„å¸‚åœºæ•°æ®DataFrameï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›None
        """
        # åŠ è½½å…ƒæ•°æ®
        metadata = self.load_metadata()
        if metadata is None or metadata.is_empty():
            return None
            
        # å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸ºæ—¥æœŸå¯¹è±¡
        if isinstance(date_val, str):
            date_obj = datetime.strptime(date_val, '%Y-%m-%d').date()
        else:
            date_obj = date_val
            
        # ç­›é€‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆä½¿ç”¨æ—¥æœŸå¯¹è±¡ï¼‰
        date_col = 'æ—¥æœŸ' if 'æ—¥æœŸ' in metadata.columns else 'date'
        day_data = metadata.filter(pl.col(date_col) == pl.lit(date_obj))
        
        if day_data.is_empty():
            return None
            
        return day_data
    
    def update_metadata(self, progress_callback=None) -> bool:
        """æ›´æ–°å¸‚åœºå…ƒæ•°æ®
        
        Args:
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°ï¼Œæ¥å—å½“å‰è¿›åº¦ã€æ€»è¿›åº¦å’Œæ¶ˆæ¯å‚æ•°
            
        Returns:
            æ›´æ–°æ˜¯å¦æˆåŠŸ
        """
        try:
            print("å¼€å§‹æ›´æ–°å¸‚åœºå…ƒæ•°æ®...")
            if progress_callback:
                progress_callback(0, 100, "å¼€å§‹æ›´æ–°å¸‚åœºå…ƒæ•°æ®")
            
            # é¦–å…ˆæ£€æŸ¥å¸‚åœºçŠ¶æ€æ•°æ®æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™é¢„è®¡ç®—
            if not os.path.exists(self.market_states_path):
                if progress_callback:
                    progress_callback(10, 100, "é¢„è®¡ç®—å¸‚åœºçŠ¶æ€æ•°æ®...")
                if not self.precompute_market_states():
                    if progress_callback:
                        progress_callback(100, 100, "é¢„è®¡ç®—å¸‚åœºçŠ¶æ€æ•°æ®å¤±è´¥ï¼Œæ— æ³•æ›´æ–°å¸‚åœºå…ƒæ•°æ®")
                    return False
            else:
                # å¢é‡æ›´æ–°å¸‚åœºçŠ¶æ€æ•°æ®
                if progress_callback:
                    progress_callback(10, 100, "å¢é‡æ›´æ–°å¸‚åœºçŠ¶æ€æ•°æ®...")
                if not self.update_market_states_incremental():
                    if progress_callback:
                        progress_callback(100, 100, "å¢é‡æ›´æ–°å¸‚åœºçŠ¶æ€æ•°æ®å¤±è´¥ï¼Œæ— æ³•æ›´æ–°å¸‚åœºå…ƒæ•°æ®")
                    return False
            
            # åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®
            if progress_callback:
                progress_callback(20, 100, "åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®...")
            market_states = self.load_market_states()
            if market_states is None or market_states.is_empty():
                if progress_callback:
                    progress_callback(100, 100, "å¸‚åœºçŠ¶æ€æ•°æ®ä¸ºç©ºï¼Œæ— æ³•æ›´æ–°å¸‚åœºå…ƒæ•°æ®")
                return False
            # ç»Ÿä¸€æ—¥æœŸç±»å‹
            market_states = _ensure_date_column(market_states, 'æ—¥æœŸ')
            
            # è·å–ç°æœ‰å¸‚åœºå…ƒæ•°æ®
            if progress_callback:
                progress_callback(30, 100, "åŠ è½½ç°æœ‰å¸‚åœºå…ƒæ•°æ®...")
            existing_metadata = self.load_metadata()
            
            # ç¡®å®šéœ€è¦æ›´æ–°çš„æ—¥æœŸèŒƒå›´
            if existing_metadata is not None and not existing_metadata.is_empty():
                # è·å–æœ€æ–°æ—¥æœŸ
                existing_metadata = _ensure_date_column(existing_metadata, 'æ—¥æœŸ')
                latest_date = existing_metadata['æ—¥æœŸ'].max()
                latest_date = _parse_to_date(latest_date)
                
                # è·å–éœ€è¦æ›´æ–°çš„æ—¥æœŸåˆ—è¡¨ï¼ˆä½¿ç”¨æ—¥æœŸå¯¹è±¡æ¯”è¾ƒï¼‰
                try:
                    dates_to_update = (
                        market_states
                        .filter(pl.col('æ—¥æœŸ') > pl.lit(latest_date))
                        ['æ—¥æœŸ']
                        .unique()
                        .sort()
                        .to_list()
                    )
                except Exception as e:
                    print(f"ç­›é€‰æ—¥æœŸæ—¶å‡ºé”™: {str(e)}")
                    # å°è¯•ä½¿ç”¨ä¸åŒçš„æ–¹æ³•è·å–æ—¥æœŸåˆ—è¡¨
                    dates_to_update = []
                    for row in market_states.to_dicts():
                        date_val = _parse_to_date(row.get('æ—¥æœŸ'))
                        if date_val and latest_date and date_val > latest_date:
                            dates_to_update.append(date_val)
                    dates_to_update = sorted(set(dates_to_update))
            else:
                # å¦‚æœæ²¡æœ‰ç°æœ‰å…ƒæ•°æ®ï¼Œè·å–æ‰€æœ‰æ—¥æœŸ
                try:
                    dates_to_update = market_states['æ—¥æœŸ'].unique().sort().to_list()
                except Exception as e:
                    print(f"è·å–æ‰€æœ‰æ—¥æœŸæ—¶å‡ºé”™: {str(e)}")
                    # å°è¯•ä½¿ç”¨ä¸åŒçš„æ–¹æ³•è·å–æ—¥æœŸåˆ—è¡¨
                    dates_to_update = []
                    for row in market_states.to_dicts():
                        date_val = _parse_to_date(row.get('æ—¥æœŸ'))
                        if date_val:
                            dates_to_update.append(date_val)
                    dates_to_update = sorted(set(dates_to_update))
                
            # å¦‚æœæ²¡æœ‰éœ€è¦æ›´æ–°çš„æ—¥æœŸï¼Œè¿”å›æˆåŠŸ
            if len(dates_to_update) == 0:
                print("å¸‚åœºå…ƒæ•°æ®å·²ç»æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")
                if progress_callback:
                    progress_callback(100, 100, "å¸‚åœºå…ƒæ•°æ®å·²ç»æ˜¯æœ€æ–°çš„ï¼Œæ— éœ€æ›´æ–°")
                return True
                
            print(f"éœ€è¦æ›´æ–° {len(dates_to_update)} ä¸ªäº¤æ˜“æ—¥çš„å¸‚åœºå…ƒæ•°æ®")
            if progress_callback:
                progress_callback(40, 100, f"éœ€è¦æ›´æ–° {len(dates_to_update)} ä¸ªäº¤æ˜“æ—¥çš„å¸‚åœºå…ƒæ•°æ®")
            
            # è®¡ç®—æ¯ä¸ªæ—¥æœŸçš„å¸‚åœºæŒ‡æ ‡
            market_stats = []
            total_dates = len(dates_to_update)
            for i, date_val in enumerate(dates_to_update):
                try:
                    print(f"å¤„ç†æ—¥æœŸ {date_val} ({i+1}/{total_dates})")
                    if progress_callback:
                        progress_callback(
                            40 + int(50 * (i+1) / total_dates),
                            100,
                            f"å¤„ç†æ—¥æœŸ {date_val} ({i+1}/{total_dates})"
                        )
                    
                    # è·å–å½“å¤©çš„å¸‚åœºçŠ¶æ€æ•°æ®
                    day_states = market_states.filter(pl.col('æ—¥æœŸ') == pl.lit(_parse_to_date(date_val)))
                    
                    # è®¡ç®—å½“å¤©çš„å¸‚åœºæŒ‡æ ‡
                    day_stats = self.calculate_daily_market_stats_from_states(day_states, _parse_to_date(date_val))
                    market_stats.append(day_stats)
                except Exception as e:
                    print(f"å¤„ç†æ—¥æœŸ {date_val} æ—¶å‡ºé”™: {str(e)}")
                    import traceback
                    traceback.print_exc()
                    # ä½¿ç”¨ç©ºçš„ç»Ÿè®¡æ•°æ®
                    market_stats.append(self._empty_stats(date_val))
            
            # å°†æ‰€æœ‰æ—¥æœŸçš„å¸‚åœºæŒ‡æ ‡åˆå¹¶ä¸ºä¸€ä¸ªDataFrame
            if progress_callback:
                progress_callback(90, 100, "åˆå¹¶å¸‚åœºæŒ‡æ ‡æ•°æ®...")
            market_stats_df = pl.DataFrame(market_stats)
            
            # åˆå¹¶æ–°æ—§å…ƒæ•°æ®
            if existing_metadata is not None and not existing_metadata.is_empty():
                if progress_callback:
                    progress_callback(95, 100, "åˆå¹¶æ–°æ—§å…ƒæ•°æ®...")
                
                # è·å–ä¸¤ä¸ªDataFrameçš„åˆ—é›†åˆ
                existing_cols = set(existing_metadata.columns)
                new_cols = set(market_stats_df.columns)
                
                # æ‰¾å‡ºå…±åŒçš„åˆ—
                common_cols = list(existing_cols.intersection(new_cols))
                
                # ç¡®ä¿è‡³å°‘æœ‰åŸºæœ¬çš„å¿…è¦åˆ—
                essential_cols = ['æ—¥æœŸ', 'æ¶¨åœæ•°', 'è·Œåœæ•°', 'ç‚¸æ¿æ•°', 'çº¢ç›˜ç‡', 'æˆäº¤æ€»é¢', 'æ€»è‚¡ç¥¨æ•°']
                missing_essential = [col for col in essential_cols if col not in common_cols]
                
                if missing_essential:
                    print(f"è­¦å‘Šï¼šåˆå¹¶å¸‚åœºå…ƒæ•°æ®æ—¶ç¼ºå°‘å¿…è¦çš„åˆ—: {missing_essential}")
                    print(f"ç°æœ‰æ•°æ®åˆ—: {existing_metadata.columns}")
                    print(f"æ–°æ•°æ®åˆ—: {market_stats_df.columns}")
                
                # å¦‚æœå…±åŒåˆ—ä¸ä¸ºç©ºï¼Œä½¿ç”¨å…±åŒåˆ—åˆå¹¶
                if common_cols:
                    print(f"ä½¿ç”¨å…±åŒåˆ—åˆå¹¶å¸‚åœºå…ƒæ•°æ®: {common_cols}")
                    # åªé€‰æ‹©å…±åŒçš„åˆ—è¿›è¡Œåˆå¹¶
                    existing_subset = existing_metadata.select(common_cols)
                    new_subset = market_stats_df.select(common_cols)
                    metadata = pl.concat([existing_subset, new_subset])
                else:
                    print("æ— æ³•æ‰¾åˆ°å…±åŒåˆ—ï¼Œä½¿ç”¨æ–°å¸‚åœºå…ƒæ•°æ®æ›¿ä»£")
                    metadata = market_stats_df
            else:
                metadata = market_stats_df
                
            # å®‰å…¨ä¿å­˜å…ƒæ•°æ®
            if progress_callback:
                progress_callback(98, 100, "ä¿å­˜å¸‚åœºå…ƒæ•°æ®...")
            if not safe_write_parquet(metadata, self.metadata_path):
                raise Exception("ä¿å­˜å¸‚åœºå…ƒæ•°æ®å¤±è´¥")
            
            print("å¸‚åœºå…ƒæ•°æ®æ›´æ–°å®Œæˆ")
            if progress_callback:
                progress_callback(100, 100, "å¸‚åœºå…ƒæ•°æ®æ›´æ–°å®Œæˆ")
            return True
        except Exception as e:
            print(f"æ›´æ–°å¸‚åœºå…ƒæ•°æ®å¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            if progress_callback:
                progress_callback(100, 100, f"æ›´æ–°å¸‚åœºå…ƒæ•°æ®å¤±è´¥: {str(e)}")
            return False
    
    def calculate_daily_market_stats_from_states(self, day_states, date_val):
        """ä»å¸‚åœºçŠ¶æ€æ•°æ®è®¡ç®—æ¯æ—¥å¸‚åœºæŒ‡æ ‡"""
        try:
            if day_states.is_empty():
                return self._empty_stats(date_val)
            
            # åŠ è½½è‚¡ç¥¨æ—¥Kå…ƒæ•°æ®
            stock_metadata = self.load_stock_metadata()
            
            # ç­›é€‰å½“å¤©çš„æ•°æ®ï¼ˆä½¿ç”¨æ—¥æœŸå¯¹è±¡ï¼‰
            day_all_stocks = stock_metadata.filter(pl.col('æ—¥æœŸ') == pl.lit(date_val))
            
            # è®¡ç®—æ€»è‚¡ç¥¨æ•°ã€æ¶¨åœæ•°ã€è·Œåœæ•°ã€ç‚¸æ¿æ•°
            stats = {
                'æ€»è‚¡ç¥¨æ•°': day_all_stocks.height if not day_all_stocks.is_empty() else 0,
                'æ¶¨åœæ•°': 0,
                'è·Œåœæ•°': 0,
                'ç‚¸æ¿æ•°': 0
            }
            
            # ä»day_statesä¸­è®¡ç®—æ¶¨åœæ•°ã€è·Œåœæ•°å’Œç‚¸æ¿æ•°
            if not day_states.is_empty():
                if 'æ¶¨åœ' in day_states.columns:
                    stats['æ¶¨åœæ•°'] = day_states.filter(pl.col('æ¶¨åœ') == True).height
                    print(f"ä»day_statesä¸­è®¡ç®—æ¶¨åœæ•°: {stats['æ¶¨åœæ•°']}")
                
                if 'è·Œåœ' in day_states.columns:
                    stats['è·Œåœæ•°'] = day_states.filter(pl.col('è·Œåœ') == True).height
                    print(f"ä»day_statesä¸­è®¡ç®—è·Œåœæ•°: {stats['è·Œåœæ•°']}")
                
                if 'ç‚¸æ¿' in day_states.columns:
                    stats['ç‚¸æ¿æ•°'] = day_states.filter(pl.col('ç‚¸æ¿') == True).height
                    print(f"ä»day_statesä¸­è®¡ç®—ç‚¸æ¿æ•°: {stats['ç‚¸æ¿æ•°']}")
            
            # å¦‚æœday_statesä¸­æ²¡æœ‰ç›¸å…³åˆ—æˆ–è®¡ç®—ç»“æœä¸º0ï¼Œå°è¯•ä»day_all_stocksä¸­è®¡ç®—
            if stats['æ¶¨åœæ•°'] == 0 and not day_all_stocks.is_empty() and 'æ¶¨åœ' in day_all_stocks.columns:
                stats['æ¶¨åœæ•°'] = day_all_stocks.filter(pl.col('æ¶¨åœ') == True).height
                print(f"ä»day_all_stocksä¸­è®¡ç®—æ¶¨åœæ•°: {stats['æ¶¨åœæ•°']}")
            
            if stats['è·Œåœæ•°'] == 0 and not day_all_stocks.is_empty() and 'è·Œåœ' in day_all_stocks.columns:
                stats['è·Œåœæ•°'] = day_all_stocks.filter(pl.col('è·Œåœ') == True).height
                print(f"ä»day_all_stocksä¸­è®¡ç®—è·Œåœæ•°: {stats['è·Œåœæ•°']}")
            
            if stats['ç‚¸æ¿æ•°'] == 0 and not day_all_stocks.is_empty() and 'ç‚¸æ¿' in day_all_stocks.columns:
                stats['ç‚¸æ¿æ•°'] = day_all_stocks.filter(pl.col('ç‚¸æ¿') == True).height
                print(f"ä»day_all_stocksä¸­è®¡ç®—ç‚¸æ¿æ•°: {stats['ç‚¸æ¿æ•°']}")
            
            # è®¡ç®—çº¢ç›˜ç‡ï¼ˆæ¶¨å¹…>0çš„è‚¡ç¥¨æ¯”ä¾‹ï¼‰
            if not day_all_stocks.is_empty() and 'æ¶¨è·Œå¹…' in day_all_stocks.columns:
                up_count = day_all_stocks.filter(pl.col('æ¶¨è·Œå¹…') > 0).height
                stats['ä¸Šæ¶¨è‚¡ç¥¨æ•°'] = up_count
                stats['çº¢ç›˜ç‡'] = (up_count / stats['æ€»è‚¡ç¥¨æ•°'] * 100) if stats['æ€»è‚¡ç¥¨æ•°'] > 0 else 0
            else:
                # å¦‚æœæ²¡æœ‰æ¶¨è·Œå¹…åˆ—ï¼Œè®¾ç½®é»˜è®¤å€¼
                stats['ä¸Šæ¶¨è‚¡ç¥¨æ•°'] = 0
                stats['çº¢ç›˜ç‡'] = 0
            
            # è®¡ç®—å¸‚åœºé‡èƒ½ï¼ˆæˆäº¤é¢ï¼‰
            # è·å–ä¸Šè¯æŒ‡æ•°å’Œæ·±è¯æˆæŒ‡çš„æˆäº¤é¢
            try:
                # è·å–æŒ‡å®šæ—¥æœŸçš„ä¸Šè¯æŒ‡æ•°å’Œæ·±è¯æˆæŒ‡æ•°æ®
                # å°è¯•ä»æŒ‡æ•°å…ƒæ•°æ®ä¸­è·å–æ•°æ®
                index_metadata_path = "data_cache/indices/index_daily_metadata.parquet"
                if os.path.exists(index_metadata_path):
                    try:
                        # è¯»å–æŒ‡æ•°å…ƒæ•°æ®
                        index_metadata = pl.read_parquet(index_metadata_path)
                        
                        # ç¡®ä¿æ—¥æœŸåˆ—æ ¼å¼æ­£ç¡®
                        date_col = 'æ—¥æœŸ'
                        
                        if date_col is not None:
                            # ç­›é€‰ä¸Šè¯æŒ‡æ•°å’Œæ·±è¯æˆæŒ‡çš„æ•°æ®
                            sh_code = '000001'
                            sz_code = '399001'
                            
                            # ç¡®å®šä»£ç åˆ—
                            code_col = 'ä»£ç '
                            
                            if code_col is not None:
                                # ç­›é€‰æŒ‡å®šæ—¥æœŸçš„æ•°æ®ï¼ˆä½¿ç”¨æ—¥æœŸå¯¹è±¡ï¼‰
                                if sh_code is not None and sz_code is not None:
                                    sh_day_data = index_metadata.filter(
                                        (pl.col(date_col) == pl.lit(date_val)) & 
                                        (pl.col(code_col) == sh_code)
                                    )
                                    
                                    sz_day_data = index_metadata.filter(
                                        (pl.col(date_col) == pl.lit(date_val)) & 
                                        (pl.col(code_col) == sz_code)
                                    )
                                    
                                    # ç¡®å®šæˆäº¤é¢åˆ—
                                    amount_col = 'æˆäº¤é¢'

                                    
                                    if amount_col is not None:
                                        # è®¡ç®—æ€»æˆäº¤é¢ï¼ˆä¸Šè¯+æ·±è¯ï¼‰
                                        sh_amount = float(sh_day_data[amount_col].sum()) if not sh_day_data.is_empty() else 0
                                        sz_amount = float(sz_day_data[amount_col].sum()) if not sz_day_data.is_empty() else 0
                                        
                                        # è½¬æ¢ä¸ºäº¿å…ƒï¼ˆä¸Šè¯+æ·±è¯ [+ åŒ—äº¤æ‰€]ï¼‰
                                        # æ³¨ï¼šakshare æŒ‡æ•°æ—¥çº¿ 'æˆäº¤é¢' ä¸ºå¯¹åº”å¸‚åœºå½“æ—¥æˆäº¤é¢ï¼Œå•ä½ä¸ºå…ƒ
                                        # å› æ­¤è¿™é‡Œä¸å†é¢å¤–ç¼©æ”¾ï¼Œç›´æ¥æŒ‰äº¿å…ƒæ±‡æ€»
                                        bj_amount = 0.0
                                        try:
                                            bj_code = '899050'
                                            bj_day_data = index_metadata.filter(
                                                (pl.col(date_col) == pl.lit(date_val)) & 
                                                (pl.col(code_col) == bj_code)
                                            )
                                            if not bj_day_data.is_empty() and 'æˆäº¤é¢' in bj_day_data.columns:
                                                bj_amount = float(bj_day_data['æˆäº¤é¢'].sum())
                                        except Exception:
                                            bj_amount = 0.0

                                        raw_amount = (sh_amount + sz_amount + bj_amount) / 100000000  # äº¿å…ƒ
                                        stats['æˆäº¤æ€»é¢'] = raw_amount
                                        print(f"ä»æŒ‡æ•°å…ƒæ•°æ®ä¸­è·å–æˆäº¤é¢: ä¸Šè¯{sh_amount/100000000:.2f}äº¿ + æ·±è¯{sz_amount/100000000:.2f}äº¿ + åŒ—è¯{bj_amount/100000000:.2f}äº¿ = {stats['æˆäº¤æ€»é¢']:.2f}äº¿")
                                        
                                        # è®¡ç®—è¿æ¿é«˜åº¦åˆ†å¸ƒ
                                        # ä½¿ç”¨è¿æ¿æ•°è€Œä¸æ˜¯è¿æ¿å¤©æ•°æ¥ç»Ÿè®¡
                                        if 'è¿æ¿æ•°' in day_states.columns:
                                            continuous_stats = day_states.filter(pl.col('è¿æ¿æ•°') > 0).select([
                                                pl.col('è¿æ¿æ•°').value_counts()
                                            ])
                                            
                                            if not continuous_stats.is_empty():
                                                continuous_dict = continuous_stats.to_dicts()[0]
                                                # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼ï¼Œç¡®ä¿é”®æ˜¯æ•´æ•°
                                                for i in range(1, 6):
                                                    # å°è¯•è·å–æ•´æ•°é”®æˆ–å­—ç¬¦ä¸²é”®
                                                    count = continuous_dict.get(i, 0)
                                                    if count == 0:
                                                        count = continuous_dict.get(str(i), 0)
                                                    stats[f'{i}è¿æ¿æ•°'] = count
                                                
                                                # å¤„ç†6è¿æ¿åŠä»¥ä¸Šçš„æƒ…å†µ
                                                six_plus_count = 0
                                                for k, v in continuous_dict.items():
                                                    try:
                                                        # å°è¯•å°†é”®è½¬æ¢ä¸ºæ•´æ•°è¿›è¡Œæ¯”è¾ƒ
                                                        if isinstance(k, str):
                                                            k_int = int(k)
                                                        else:
                                                            k_int = k
                                                        
                                                        if k_int >= 6:
                                                            six_plus_count += v
                                                    except (ValueError, TypeError):
                                                        # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè·³è¿‡è¯¥é”®
                                                        continue
                                                
                                                stats['6è¿æ¿æ•°'] = six_plus_count
                                            else:
                                                # å¦‚æœæ²¡æœ‰è¿æ¿æ•°æ®ï¼Œè®¾ç½®é»˜è®¤å€¼
                                                for i in range(1, 6):
                                                    stats[f'{i}è¿æ¿æ•°'] = 0
                                                stats['6è¿æ¿æ•°'] = 0
                                        else:
                                            # å¦‚æœæ²¡æœ‰è¿æ¿æ•°åˆ—ï¼Œè®¾ç½®é»˜è®¤å€¼
                                            for i in range(1, 6):
                                                stats[f'{i}è¿æ¿æ•°'] = 0
                                            stats['6è¿æ¿æ•°'] = 0
                                        
                                        # æ·»åŠ æ—¥æœŸ
                                        stats['æ—¥æœŸ'] = date_val
                                        
                                        return stats
                    except Exception as e:
                        print(f"ä»æŒ‡æ•°å…ƒæ•°æ®è·å–å¸‚åœºé‡èƒ½æ—¶å‡ºé”™: {str(e)}")
                
                # å¦‚æœæ— æ³•ä»æŒ‡æ•°å…ƒæ•°æ®è·å–ï¼Œå°è¯•ä»å¸‚åœºçŠ¶æ€æ•°æ®ä¸­è·å–æˆäº¤é¢
                if 'æˆäº¤é¢' in day_states.columns:
                    stats['æˆäº¤æ€»é¢'] = day_states['æˆäº¤é¢'].sum() / 100000000
                    print(f"ä»å¸‚åœºçŠ¶æ€æ•°æ®ä¸­è·å–æˆäº¤é¢: {stats['æˆäº¤æ€»é¢']} äº¿å…ƒ")
                elif 'æˆäº¤é¢' in day_all_stocks.columns:
                    stats['æˆäº¤æ€»é¢'] = day_all_stocks['æˆäº¤é¢'].sum() / 100000000
                    print(f"ä»æ—¥Kæ•°æ®ä¸­è·å–æˆäº¤é¢: {stats['æˆäº¤æ€»é¢']} äº¿å…ƒ")
                else:
                    stats['æˆäº¤æ€»é¢'] = 0
                    print("æ— æ³•è·å–æˆäº¤é¢ï¼Œè®¾ç½®ä¸º0")
                
                # è®¡ç®—è¿æ¿é«˜åº¦åˆ†å¸ƒ
                # ä½¿ç”¨è¿æ¿æ•°è€Œä¸æ˜¯è¿æ¿å¤©æ•°æ¥ç»Ÿè®¡
                if 'è¿æ¿æ•°' in day_states.columns:
                    continuous_stats = day_states.filter(pl.col('è¿æ¿æ•°') > 0).select([
                        pl.col('è¿æ¿æ•°').value_counts()
                    ])
                    
                    if not continuous_stats.is_empty():
                        continuous_dict = continuous_stats.to_dicts()[0]
                        # è½¬æ¢ä¸ºæ‰€éœ€æ ¼å¼ï¼Œç¡®ä¿é”®æ˜¯æ•´æ•°
                        for i in range(1, 6):
                            # å°è¯•è·å–æ•´æ•°é”®æˆ–å­—ç¬¦ä¸²é”®
                            count = continuous_dict.get(i, 0)
                            if count == 0:
                                count = continuous_dict.get(str(i), 0)
                            stats[f'{i}è¿æ¿æ•°'] = count
                        
                        # å¤„ç†6è¿æ¿åŠä»¥ä¸Šçš„æƒ…å†µ
                        six_plus_count = 0
                        for k, v in continuous_dict.items():
                            try:
                                # å°è¯•å°†é”®è½¬æ¢ä¸ºæ•´æ•°è¿›è¡Œæ¯”è¾ƒ
                                if isinstance(k, str):
                                    k_int = int(k)
                                else:
                                    k_int = k
                                
                                if k_int >= 6:
                                    six_plus_count += v
                            except (ValueError, TypeError):
                                # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œè·³è¿‡è¯¥é”®
                                continue
                        
                        stats['6è¿æ¿æ•°'] = six_plus_count
                    else:
                        # å¦‚æœæ²¡æœ‰è¿æ¿æ•°æ®ï¼Œè®¾ç½®é»˜è®¤å€¼
                        for i in range(1, 6):
                            stats[f'{i}è¿æ¿æ•°'] = 0
                        stats['6è¿æ¿æ•°'] = 0
                else:
                    # å¦‚æœæ²¡æœ‰è¿æ¿æ•°åˆ—ï¼Œè®¾ç½®é»˜è®¤å€¼
                    for i in range(1, 6):
                        stats[f'{i}è¿æ¿æ•°'] = 0
                    stats['6è¿æ¿æ•°'] = 0
            except Exception as e:
                print(f"è®¡ç®—å¸‚åœºé‡èƒ½æ—¶å‡ºé”™: {str(e)}")
                stats['æˆäº¤æ€»é¢'] = 0
                for i in range(1, 7):
                    stats[f'{i}è¿æ¿æ•°'] = 0
            
            # æ·»åŠ æ—¥æœŸ
            stats['æ—¥æœŸ'] = date_val
            
            return stats
        except Exception as e:
            print(f"è®¡ç®—å¸‚åœºæŒ‡æ ‡æ—¶å‡ºé”™: {str(e)}")
            import traceback
            traceback.print_exc()
            return self._empty_stats(date_val)

    def is_latest_daily_trading_day(self) -> bool:
        """æ£€æŸ¥å¸‚åœºçŠ¶æ€æ•°æ®æ˜¯å¦ä¸ºæœ€æ–°äº¤æ˜“æ—¥"""
        try:
            # åŠ è½½å¸‚åœºçŠ¶æ€æ•°æ®
            market_states = self.load_market_states()
            if market_states is None or market_states.is_empty():
                print("å¸‚åœºçŠ¶æ€æ•°æ®ä¸ºç©ºï¼Œéœ€è¦æ›´æ–°")
                return False

            # è·å–æ•°æ®ä¸­çš„æœ€æ–°æ—¥æœŸ
            latest_date = market_states['æ—¥æœŸ'].max()

            # ç¡®ä¿latest_dateæ˜¯datetime.dateç±»å‹
            if isinstance(latest_date, str):
                latest_date = datetime.strptime(latest_date, '%Y-%m-%d').date()
            elif hasattr(latest_date, 'date'):
                latest_date = latest_date.date()

            # è·å–å½“å‰æ—¥æœŸ
            today = datetime.now().date()

            # æ£€æŸ¥ä»Šå¤©æ˜¯å¦æ˜¯äº¤æ˜“æ—¥
            is_weekend = today.weekday() >= 5  # å‘¨æœ«
            try:
                from utils.visualizer import CHINA_HOLIDAYS
                is_holiday = today.strftime('%Y-%m-%d') in CHINA_HOLIDAYS
            except:
                is_holiday = False

            is_trading_day = not (is_weekend or is_holiday)

            # å¦‚æœä»Šå¤©æ˜¯äº¤æ˜“æ—¥ï¼Œæ£€æŸ¥æ•°æ®æ˜¯å¦åŒ…å«ä»Šå¤©
            if is_trading_day:
                return latest_date >= today
            else:
                # å¦‚æœä»Šå¤©ä¸æ˜¯äº¤æ˜“æ—¥ï¼Œæ‰¾åˆ°æœ€è¿‘çš„äº¤æ˜“æ—¥
                check_date = today - timedelta(days=1)
                while check_date.weekday() >= 5 or (check_date.strftime('%Y-%m-%d') in getattr(self, '_holidays', [])):
                    check_date -= timedelta(days=1)
                return latest_date >= check_date

        except Exception as e:
            print(f"æ£€æŸ¥å¸‚åœºçŠ¶æ€æ•°æ®æœ€æ–°æ—¥æœŸå¤±è´¥: {e}")
            return False
